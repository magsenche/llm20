{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pathlib\n",
    "import re\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from submission.utils import default, prompt\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "\n",
    "def format_message(role: str, content: str) -> dict:\n",
    "    return {\"role\": role, \"content\": content}\n",
    "\n",
    "\n",
    "KAGGLE_AGENT = pathlib.Path(\"/kaggle_simulations/agent\")\n",
    "\n",
    "if KAGGLE_AGENT.exists():\n",
    "    model = \"llama-3.1/transformers/8b-instruct/1\"\n",
    "    model_path = KAGGLE_AGENT / \"input\" / model\n",
    "\n",
    "    pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_path,\n",
    "        model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    def call_model(\n",
    "        messages: list[dict],\n",
    "        max_new_tokens: int = 16,\n",
    "        temperature: int = 0.6,\n",
    "    ) -> dict[dict[str]]:\n",
    "        outputs = pipeline(\n",
    "            messages, max_new_tokens=max_new_tokens, temperature=temperature\n",
    "        )\n",
    "        return outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "\n",
    "else:\n",
    "    import ollama\n",
    "\n",
    "    model = \"llama3.1:8b\"\n",
    "\n",
    "    def call_model(\n",
    "        messages: list[dict],\n",
    "        max_new_tokens: int = 16,\n",
    "        temperature: int = 0.6,\n",
    "    ) -> dict[dict[str]]:\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            options={\"num_predict\": max_new_tokens, \"temperature\": temperature},\n",
    "        )\n",
    "        return response[\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    max_new_token: int = 32\n",
    "    temperature: int = 0.6\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        turn_types: list[str] = [],\n",
    "        sys_prompt: str = \"You are an AI agent\",\n",
    "    ) -> None:\n",
    "        self.turn_types = turn_types\n",
    "        self.sys_prompt = sys_prompt\n",
    "        self.state = []\n",
    "\n",
    "    def __call__(self, obs, cfg) -> str:\n",
    "        if obs.turnType in self.turn_types:\n",
    "            self.config_state(obs)\n",
    "            return self.result(obs, cfg)\n",
    "        else:\n",
    "            raise KeyError\n",
    "\n",
    "    def result(self, obs, cfg) -> str:\n",
    "        response = call_model(self.state, self.max_new_token, self.temperature)\n",
    "        res = self.check(response)\n",
    "        return res\n",
    "\n",
    "    def config_state(self, obs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def check(self, response: str) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reset_state(self) -> None:\n",
    "        self.state = []\n",
    "        self.update_state(\"system\", self.sys_prompt)\n",
    "\n",
    "    def update_state(self, role: str, content: str | None) -> None:\n",
    "        if content is not None and len(content) > 0:\n",
    "            message = format_message(role, content)\n",
    "            self.state.append(message)\n",
    "\n",
    "\n",
    "class Questioner(Agent):\n",
    "    max_new_token: int = 128\n",
    "\n",
    "    def __init__(self, turn_types: str = [\"ask\", \"guess\"]) -> None:\n",
    "        super().__init__(turn_types=turn_types, sys_prompt=prompt.questioner)\n",
    "\n",
    "    def config_state(self, obs):\n",
    "        self.reset_state()\n",
    "        self.update_state(\"user\", \"Let's play !\")\n",
    "        for q, a in itertools.zip_longest(obs.questions, obs.answers):\n",
    "            self.update_state(\"assistant\", q)\n",
    "            self.update_state(\"user\", a)\n",
    "\n",
    "\n",
    "class Asker(Questioner):\n",
    "    temperature: int = 0.6\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(turn_types=[\"ask\"])\n",
    "        self.questions = default.questions\n",
    "\n",
    "    def next_question(self) -> str:\n",
    "        return self.questions.pop(0)\n",
    "\n",
    "    def result(self, obs, cfg) -> str:\n",
    "        if len(obs.questions) == 0:\n",
    "            res = self.next_question()\n",
    "        else:\n",
    "            res = super().result(obs, cfg)\n",
    "        return res\n",
    "\n",
    "    def config_state(self, obs):\n",
    "        super().config_state(obs)\n",
    "        i = len(obs.questions) + 1\n",
    "        if obs.answers[-3:] == 3 * [\"no\"]:\n",
    "            ask_prompt = prompt.ask_3no.format(i=i)\n",
    "        else:\n",
    "            ask_prompt = prompt.ask.format(i=i)\n",
    "        self.update_state(\"user\", ask_prompt)\n",
    "\n",
    "    def check(self, response: str) -> str:\n",
    "        print(f\"\\033[91m{response}\\033[0m\")\n",
    "        matched = re.match(r\".*\\?$\", response)\n",
    "        return response if matched else self.next_question()\n",
    "\n",
    "\n",
    "class Guesser(Questioner):\n",
    "    temperature: int = 0.5\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(turn_types=[\"guess\"])\n",
    "        self.guesses = default.guesses\n",
    "\n",
    "    def next_guess(self) -> str:\n",
    "        return self.guesses.pop(0)\n",
    "\n",
    "    def result(self, obs, cfg) -> str:\n",
    "        start_time = time.time()\n",
    "        while time.time() - start_time < 0.8 * cfg.actTimeout:\n",
    "            res = super().result(obs, cfg).lower().strip()\n",
    "            if res not in obs.guesses and res.count(\" \") < 3:\n",
    "                return res\n",
    "            else:\n",
    "                self.temperature = min(0.9, self.temperature + 0.05)\n",
    "        return self.next_guess()\n",
    "\n",
    "    def reset_state(self):\n",
    "        super().reset_state()\n",
    "        self.temperature = Guesser.temperature\n",
    "\n",
    "    def config_state(self, obs):\n",
    "        super().config_state(obs)\n",
    "        i = len(obs.guesses) + 1\n",
    "        self.update_state(\"user\", prompt.guess.format(i=i, guesses=obs.guesses))\n",
    "\n",
    "    def check(self, response: str) -> str:\n",
    "        print(f\"\\033[92m{response}\\033[0m\")\n",
    "        res = re.findall(r\"\\*{2,}(.*?)\\*{2,}\", response.replace(\"\\n\", \"\"))\n",
    "        return res[-1] if res else self.next_guess()\n",
    "\n",
    "\n",
    "class Answerer(Agent):\n",
    "    max_new_token: int = 16\n",
    "    temperature: int = 0.1\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(turn_types=[\"answer\"], sys_prompt=prompt.answerer)\n",
    "\n",
    "    def config_state(self, obs):\n",
    "        self.reset_state()\n",
    "        self.update_state(\n",
    "            \"user\",\n",
    "            prompt.answer.format(\n",
    "                keyword=obs.keyword, category=obs.category, question=obs.questions[-1]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def check(self, response: str) -> str:\n",
    "        print(f\"\\033[93m{response}\\033[0m\")\n",
    "        return \"yes\" if \"yes\" in response.lower() else \"no\"\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "\n",
    "    match obs.turnType:\n",
    "        case \"ask\":\n",
    "            return Asker()(obs, cfg)\n",
    "        case \"answer\":\n",
    "            if len(obs.guesses) == 0:\n",
    "                print(f\"\\033[93m{obs.keyword}\\033[0m\")\n",
    "            return Answerer()(obs, cfg)\n",
    "        case \"guess\":\n",
    "            return Guesser()(obs, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle_environments\n",
    "\n",
    "env = kaggle_environments.make(environment=\"llm_20_questions\", debug=True)\n",
    "game_output = env.run(agents=[agent_fn, agent_fn, \"random_guesser\", \"random_answerer\"])\n",
    "\n",
    "env.render(mode=\"ipython\", width=1080, height=700)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
